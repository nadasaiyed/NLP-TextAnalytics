After training our Word2Vec skip-gram model to reconstruct linguistic contexts of words, we find that not all words similar to "good" are positive and same is the case for "bad". This could be because Skip-gram predicts surrounding context words from the target words. So, words like bad might have a positive context like "not bad" - hence similar to good. Similarly, if choosing CBOW, it predicts target words from the surrounding context words. So when different words (semantically different individually) are similar in context, then Word2Vec will have similar outputs when these words are passed as inputs, that is the computed word vectors (in the hidden layer) for these words will be similar.
